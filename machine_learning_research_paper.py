# -*- coding: utf-8 -*-
"""Machine Learning Research Paper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rA-vFMjWq_vR2s_hJ1Y97kAXsgIeJjeg
"""

import os
import nltk
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
from nltk.stem import wordnet 
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
import nltk
nltk.download("stopwords")
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import regex as re

import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS
import seaborn as sns
from sklearn.metrics import classification_report

!pip install spacy
!python -m spacy download en_core_web_md

from nltk.tokenize import word_tokenize

df_fake = pd.read_csv("/Fake.csv")
df_real = pd.read_csv("/True.csv")

df_fake['category'] = 0
df_real['category'] = 1

df = [df_fake, df_real]
#concatenate the "fake" and "real" data set 
df= pd.concat(df)

#spliting the data into 1/100th of the size so that it allows for a faster run time(smaller model)
#random sample 
df_sample = df.sample(frac = 0.01 , replace =False, random_state=42)

df_sample

#stopwords removal
stop_words = set(stopwords.words('english'))

import string 
punctuation = list(string.punctuation)
#update the stop_words to include punctuation as well
stop_words.update(punctuation)



df_sample['without_stopwords_text'] = df_sample['text'].apply(lambda x:' '.join([word for word in x.split() if word not in  stop_words ]))

#Data Cleaning:
tokenizer = nltk.tokenize.WhitespaceTokenizer()

def remove_square_brackets(text):
  return re.sub('\[[^]]*\]', '', text)


def remove_stopwords(text):
  text_list=[]
  for i in text.split():
    if i.strip().lower()not in stop_words:
      text_list.append(i.strip())
  return " ".join(text_list) 

def wordrop(text):
  text = text.lower()
  text = re.sub('\[.*?\]', '', text)
  text = re.sub("\\W"," ",text) 
  text = re.sub('https?://\S+|www\.\S+', '', text)
  text = re.sub('<.*?>+', '', text)
  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
  text = re.sub('\w*\d\w*', '', text)   
  return text


def data_cleansing(text):
  text = wordrop(text)
  text = remove_square_brackets(text)
  text = remove_stopwords(text)
  return text

df_sample ['without_stopwords_text']  = df_sample['without_stopwords_text'].apply(data_cleansing)

print(df_sample['without_stopwords_text'].iloc[1])

#get each row into a list of strings
text_column = df_sample['without_stopwords_text']
text_arr=[]
text_arr = np.append(text_arr, text_column)
text_arr

category_column = df_sample['category']
category_arr = []
category_arr = np.append(category_arr, category_column)

#tokenize all the text and spliting into a list 
tokenized_row_list = []
for row_list in text_arr:
  tokenized_row_list.append(word_tokenize(row_list))

#spliting data
x_train, x_test, y_train,y_test = sklearn.model_selection.train_test_split(text_arr, category_arr,test_size =0.25 ,random_state = 42)

#getting the text from the first text in the column
first_text = df_sample['without_stopwords_text'].iloc[0]
print(first_text)

"""
#count the number of times each word showing up in the token
first_text_token_list = FreqDist()
for word in first_text_token:
  first_text_token_list[word.lower()]+=1
"""

#Compare the "true" dataset & "fake" dataset frequency distribution
text_token_list = FreqDist()
for word_list in tokenized_row_list:
  for word in word_list:
    text_token_list[word.lower()]+=1

#most common 50 words in the token list
text_token_list.most_common(50)

#top 10 frequency in a list 
text_token_list_top10 = text_token_list.most_common(10)
text_token_list_top10

#stemming and normalize the words back to its root form 
#initialize the porter stemmer
pst = PorterStemmer()
#test "word" in stemmer
pst.stem('insanity')

#lemmatization in practice, getting words back to the root form 

#remove "."
tokenizer = nltk.tokenize.WhitespaceTokenizer()
#Lemmatization , it would put a word, and group a bunch of words to the same form, let say they map go, went, gone to go
lemmatizer = WordNetLemmatizer()


def lemmatization(text):
  return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)]


df_sample['without_stopwords_text_tokenized'] = df_sample['without_stopwords_text'].apply(lemmatization)

plt.figure(figsize = (20,20)) # Text that is not Fake
wordcloud_real = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(" ".join(df_sample[df_sample.category == 1].text))
plt.imshow(wordcloud_real , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is not Fake
wordcloud_fake = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(" ".join(df_sample[df_sample.category == 0].text))
plt.imshow(wordcloud_fake , interpolation = 'bilinear')

#sentiment analysis: 
from textblob import TextBlob

def sentiment(text):
  try:
    return TextBlob(text).sentiment
  except: 
    return None
df_sample['sentiment']= df_sample['without_stopwords_text'].apply(sentiment)

df_sample['sentiment']#polarity, subjectivity

#get all the words in the text and see which one are the most popular
def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words


corpus = get_corpus(df_sample['without_stopwords_text'])

#use the counter lilbrary to find the most_common in the whole text, so that we can use this for n-gram analysis
from collections import Counter
counter = Counter(corpus)
most_common = counter.most_common(10)
most_common = dict(most_common)
most_common

#Tokenization: Ngram + Bigram + Trigram + unigram (tokens of written word) for the whole text

from nltk.util import bigrams, trigrams, ngrams
from sklearn.feature_extraction.text import CountVectorizer

#creating a function where we can apply to 3 different analysis by just calling the function


#n is the number of most frequent word
#g is to define whether if it is trigram/bigram/unigram
#corpus the whole text
def n_gram_analysis(corpus, n, g): 
  vector_text = CountVectorizer(ngram_range=(g,g)).fit(corpus) #determine which type of analysis
  x = vector_text.transform(corpus) #vectorized matrix (bag of words)
  sum_of_words = x.sum(axis = 0) #sum all the words in the horiontal axis
  word_frequency = [(word,sum_of_words[0,i]) for word, i in vector_text.vocabulary_.items()] #for teach word count the frequency 
  word_frequency = sorted(word_frequency, key = lambda x:x[1],reverse = True) #sort it in a descending order by creating a lambda function
  return word_frequency[:n]#return everything from beginning to n

#Unigram
plt.figure(figsize = (16,9))
#find the most frequent 10 words using unigram 
top_10_uni = n_gram_analysis(df_sample['without_stopwords_text'],10,1)
#use dictionary so we could use the keys for the x-axis, and values for the y axis
top_10_uni = dict(top_10_uni)
sns.barplot(x=list(top_10_uni.values()),y=list(top_10_uni.keys()))

#Bigram
plt.figure(figsize = (16,9))
#find the most frequent 10 words using bigram 
top_10_bi = n_gram_analysis(df_sample['without_stopwords_text'],10,2)
#use dictionary so we could use the keys for the x-axis, and values for the y axis
top_10_bi = dict(top_10_bi)
sns.barplot(x=list(top_10_bi.values()),y=list(top_10_bi.keys()))

#Trigram

plt.figure(figsize = (16,9))
#find the most frequent 10 words using trigram 
top_10_tri = n_gram_analysis(df_sample['without_stopwords_text'],10,3)
#use dictionary so we could use the keys for the x-axis, and values for the y axis
top_10_tri = dict(top_10_tri)
sns.barplot(x=list(top_10_tri.values()),y=list(top_10_tri.keys()))

#bag of word and SVM classifer 
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(binary = True)
train_x_vector = vectorizer.fit_transform(x_train)

from sklearn import svm
classify_svm = svm.SVC(kernel = 'linear')
classify_svm.fit(train_x_vector, y_train)

test_x_vector = vectorizer.transform(x_test)
predicted = classify_svm.predict(test_x_vector)

#Return the mean accuracy on the given test data and labels.

print(classify_svm.score (train_x_vector, y_train))
print(classify_svm.score(test_x_vector,y_test))
print(classification_report(y_test, predicted))

#Random Forest

from sklearn.ensemble import RandomForestClassifier
random_forest  = RandomForestClassifier(random_state = 42)
random_forest.fit(train_x_vector,y_train)
predicted_random_forest = random_forest.predict(test_x_vector)

print(random_forest.score(test_x_vector,y_test))

print(classification_report(y_test, predicted_random_forest))

